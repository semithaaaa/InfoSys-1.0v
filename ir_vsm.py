# -*- coding: utf-8 -*-
"""IR_VSM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okR-CoNnRZOz-izP1qOS5af2E26XQbhG
"""

# !pip install dash
# !pip install dash-html-components
# !pip install dash-core-components
# !pip install plotly
# !pip install beautifulsoup4
# !pip install requests


#####################################################################################################

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import os
import re
import string
import json
import nltk
from collections import defaultdict, Counter
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import pandas as pd
from collections import Counter

import dash
from dash import dcc, html, Input, Output, State
from dash.dependencies import Input, Output
import dash_bootstrap_components as dbc
import plotly.express as px

nltk.download('stopwords')
nltk.download('punkt')


#####################################################################################################

visited_urls = set()
output_dir = "crawled_pages"

def crawl(seed_url, max_pages=250, delay=1, fail_delay=3, output_dir=output_dir):
    """Crawls web pages starting from a seed URL, up to a maximum number of pages."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    queue = [seed_url]
    page_count = 0

    while queue and page_count < max_pages:
        url = queue.pop(0)
        if url not in visited_urls:
            try:
                response = requests.get(url, timeout=10)
                response.raise_for_status()

                save_page(url, response.text, output_dir)

                visited_urls.add(url)
                page_count += 1

                print(f"Page {page_count}: {url} crawled successfully.")

                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    absolute_url = urljoin(url, link['href'])

                    if is_valid(absolute_url, seed_url) and absolute_url not in visited_urls:
                        queue.append(absolute_url)

                time.sleep(2)

            except requests.RequestException as e:
                print(f"Failed to crawl {url}: {e}")
                print(f"Resting for {fail_delay} seconds before retrying.")
                time.sleep(fail_delay)
                continue


def save_page(url, content, output_dir):
    """Saves the content of a web page to a file in the specified directory."""
    parsed_url = urlparse(url)
    file_name = f"{parsed_url.netloc}{parsed_url.path}".replace("/", "_").strip("_") + ".html"
    file_path = os.path.join(output_dir, file_name)

    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)


def is_valid(url, seed_url):
    """Checks if a URL is valid for crawling (same domain as seed URL)."""
    return urlparse(url).netloc == urlparse(seed_url).netloc

if __name__ == "__main__":
    seed_page = "https://www.ted.com/talks"
    crawl(seed_page, max_pages=250, delay=1, fail_delay=3)

"""## Indexing"""

import os
import json
import nltk
from bs4 import BeautifulSoup
from collections import defaultdict
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

# Initialize stemmer and stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Function to normalize text
def normalize(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Convert to lowercase, remove punctuation, and apply stemming
    normalized_tokens = [
        stemmer.stem(token.lower())
        for token in tokens
        if token.isalpha() and token.lower() not in stop_words  # Remove stopwords and punctuation
    ]

    return normalized_tokens

# Create inverted index
def create_inverted_index(folder_path):
    inverted_index = defaultdict(list)

    # Loop through all .html files in the folder
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".html"):
            file_path = os.path.join(folder_path, file_name)

            # Open and parse the HTML file
            with open(file_path, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f, 'html.parser')

                # Extract text content from HTML
                text = soup.get_text()

                # Normalize and tokenize text
                tokens = normalize(text)

                # Add tokens to the inverted index
                for token in tokens:
                    inverted_index[token].append(file_name)

    return inverted_index

# Store inverted index in a file
def save_index(inverted_index, output_file):
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(inverted_index, f)

# Folder containing crawled web pages (saved as .html files)
folder_path = 'crawled_pages'

# Create the inverted index
inverted_index = create_inverted_index(folder_path)

# Save the inverted index to a file (JSON format)
output_file = 'inverted_index.json'
save_index(inverted_index, output_file)

print(f"Inverted index saved to {output_file}")



def load_corpus(crawled_pages_dir="crawled_pages"):
    corpus = []
    file_names = []
    for file_name in os.listdir(crawled_pages_dir):
        if file_name.endswith(".html"):
            file_path = os.path.join(crawled_pages_dir, file_name)
            with open(file_path, "r", encoding="utf-8") as file:
                content = file.read()
                corpus.append(content)
                file_names.append(file_name)
    return corpus, file_names

def calculate_tfidf_matrix(corpus):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    return tfidf_matrix, vectorizer

def search_vsm(query, tfidf_matrix, vectorizer, file_names):
    query_vec = vectorizer.transform([query])
    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
    ranked_results = sorted(zip(file_names, similarities), key=lambda x: x[1], reverse=True)
    return ranked_results

# Example usage
corpus, file_names = load_corpus()
tfidf_matrix, vectorizer = calculate_tfidf_matrix(corpus)
query = "Climate Change"
results = search_vsm(query, tfidf_matrix, vectorizer, file_names)
print(results)

results_df = pd.DataFrame(results, columns=['File Name', 'Similarity Score'])
results_df


# Calculate term probabilities for each document and the entire corpus
def calculate_qlm_probabilities(corpus):
    doc_term_probs = []
    collection_counts = Counter()
    total_terms_collection = 0

    # Process each document to create term probability distributions
    for doc in corpus:
        tokens = word_tokenize(doc)
        total_terms_collection += len(tokens)
        collection_counts.update(tokens)

        total_terms = len(tokens)
        term_counts = Counter(tokens)
        term_probs = {term: count / total_terms for term, count in term_counts.items()}
        doc_term_probs.append(term_probs)

    # Calculate collection term probabilities
    collection_term_probs = {term: count / total_terms_collection for term, count in collection_counts.items()}
    return doc_term_probs, collection_term_probs

# Search using Jelinek-Mercer smoothing
def search_qlm_jm(query, doc_term_probs, collection_term_probs, file_names, lambda_param=0.2):
    query_tokens = word_tokenize(query)
    scores = []

    for doc_index, term_probs in enumerate(doc_term_probs):
        score = 0
        for term in query_tokens:
            doc_term_prob = term_probs.get(term, 0)
            collection_term_prob = collection_term_probs.get(term, 0)

            # Apply Jelinek-Mercer smoothing
            smoothed_prob = (lambda_param * doc_term_prob) + ((1 - lambda_param) * collection_term_prob)
            score += np.log(smoothed_prob + 1e-10)  # Add small value to avoid log(0)

        scores.append((file_names[doc_index], score))

    ranked_results = sorted(scores, key=lambda x: x[1], reverse=True)
    return ranked_results

# Example usage
# corpus = load_corpus() # Already loaded corpus content
doc_term_probs, collection_term_probs = calculate_qlm_probabilities(corpus)
results_qlm_jm = search_qlm_jm(query, doc_term_probs, collection_term_probs, file_names, lambda_param=0.2)
print(results_qlm_jm)


#################################################################################
